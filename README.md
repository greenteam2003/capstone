Gather, a place to hang out with your friends. 

Demo: https://www.youtube.com/watch?v=MVvf3Z2GoKk&feature=youtu.be

Gather is a variation on video conferencing. Inspired by quarantine, it is a program that lets you virtually hang out with your friends in fun locations, like at a picnic or in Paris. The final product looks like Zoom’s gallery view (i.e., you see everybody in the same window), but there is a customizable backdrop behind all of the video streams. The backgrounds are removed from each video stream as well, so it looks like each participant is right there inside of the backdrop scene. You can also drag and drop the video streams to be anywhere on the backdrop. For example, if the backdrop is a stock photo of friends at a barbecue, you can drag your friends’ videos to be overlaid on top of each stock photo person to make it look like your friends are actually at this barbecue.

We extended the video conferencing server provided by Twilio to create our project. This server takes a participant’s webcam feed and distributes it to everyone else in the video chat room. The challenge here was that we wanted to remove every participant’s background before distributing it. We found the place in the code where the webcam is accessed by Twilio, then re-routed the webcam feed to a new script that removes the background. In this script, for each video frame, we paint the image from the video frame onto a hidden HTML5 canvas. We used a TensorFlow model called BodyPix to loop through each pixel of that canvas and identify which pixels belong to a person. We then take the output from BodyPix and render the image onto a second canvas, replacing each non-person pixel (the background) with a bright green pixel. At this point, the image would look like a person sitting in front of a greenscreen. 

Ideally, rather than replacing the participant’s background with bright green, we would have only copied over the pixels that make up a person, leaving the canvas background transparent. Unfortunately, Twilio does not support the opacity channel when serving video to participants (they only support RGB), so instead of displaying video feeds with a transparent background, it would have appeared as a black background on the receiving end. We chose to replace the background with bright green because that color is easily identified and removed in a process that is described later.

Back to the flow of data: We now have an image of a person with a green background rendered on a canvas. The function that renders this image from a video frame is called recursively each time the browser encounters an idle period in order to improve performance. The changing images on the canvas would look like a video stream, but we cannot yet use that information alone in Twilio, which requires an actual video stream. We used canvas’s captureStream feature to convert the changing canvas images back into a video stream, which we feed back into Twilio to distribute to each participant in the room. 

On the receiving end, we had to remove the green background from the video stream before displaying it to participants. We used a very similar canvas setup to achieve this. Each video stream is recursively rendered on its own hidden HTML5 canvas, then we loop through pixel by pixel and paint only the non-green pixels onto a second canvas, which leaves the background transparent. We use captureStream again to turn the changing canvas into a video stream that is rendered inside of a React Component. 

Each video component is wrapped in a React-Draggable.js component, which allows end users to click and drag video feeds, moving them around the screen in their local view. As soon as a user releases their click-and-drag, the new coordinates of the Draggable component are sent to Firebase, which updates the coordinates for that participant’s component on everyone’s machine, keeping everyone in sync. The backdrop of the room is also synced up via Firebase. Users can paste in an image url or select from a dropdown list.

